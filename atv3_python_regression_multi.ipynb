{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFbJX9Vu4Go7dY9+FQVDZq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonioFialhoSN/atv3_python_regression_multi/blob/main/atv3_python_regression_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redes Neurais"
      ],
      "metadata": {
        "id": "VHwGt09blz43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute_cost_multi.py\n"
      ],
      "metadata": {
        "id": "pzoPXcVzkdoY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m6TJVXzWkRgr"
      },
      "outputs": [],
      "source": [
        "# Functions/compute_cost_multi.py\n",
        "\"\"\"\n",
        "@file compute_cost_multi.py\n",
        "@brief Computes the cost for multivariate linear regression.\n",
        "@details Este módulo contém uma função para calcular o custo de um modelo de regressão linear\n",
        "          multivariada utilizando a função de custo de erro quadrático médio.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_cost_multi(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calcula o custo para regressão linear multivariada.\n",
        "\n",
        "    A função de custo é definida como:\n",
        "        J(θ) = (1 / (2m)) * (Xθ - y)ᵀ (Xθ - y)\n",
        "\n",
        "    :param (ndarray) X: Matriz de features incluindo o termo de intercepto (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :param (ndarray) theta: Vetor de parâmetros (shape: n+1,).\n",
        "    :return (float): Valor do custo calculado.\n",
        "    \"\"\"\n",
        "    # get the number of training examples\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # compute the predictions using the linear model by formula h(θ) = X @ θ\n",
        "    predictions = X @ theta\n",
        "\n",
        "    # compute the error vector between predictions and actual values\n",
        "    errors = predictions - y\n",
        "\n",
        "    # compute the cost as the mean squared error cost function\n",
        "    cost = (1 / (2 * m)) * np.dot(errors, errors)\n",
        "\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##features_normalize.py"
      ],
      "metadata": {
        "id": "K8n23IYtkelf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/feature_normalize.py\n",
        "\"\"\"\n",
        "@file features_normalizes.py\n",
        "@brief Funções para normalização de features em datasets.\n",
        "@details Este módulo contém funções para normalizar as features de um dataset\n",
        "          utilizando diferentes abordagens, como média e desvio padrão, ou\n",
        "          mínimo e máximo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def features_normalize_by_std(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para média zero e desvio padrão unitário.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - mu) / sigma\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - mu é o vetor de médias (1 x n) de cada feature.\n",
        "        - sigma é o vetor de desvios padrão (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tripla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - mu (ndarray): Vetor com as médias de cada feature.\n",
        "        - sigma (ndarray): Vetor com os desvios padrão de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula a média de cada feature (coluna)\n",
        "    mu = np.mean(X, axis=0)\n",
        "\n",
        "    # Calcula o desvio padrão de cada feature (coluna)\n",
        "    sigma = np.std(X, axis=0)\n",
        "\n",
        "    # Evita divisão por zero\n",
        "    sigma[sigma == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return X_norm, mu, sigma\n",
        "\n",
        "\n",
        "def features_normalizes_by_min_max(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para o intervalo [0, 1] utilizando o mínimo e o máximo.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - min) / (max - min)\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - min é o vetor de mínimos (1 x n) de cada feature.\n",
        "        - max é o vetor de máximos (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tupla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - min (ndarray): Vetor com os valores mínimos de cada feature.\n",
        "        - max (ndarray): Vetor com os valores máximos de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula o mínimo de cada feature (coluna)\n",
        "    min = np.min(X, axis=0)\n",
        "\n",
        "    # Calcula o máximo de cada feature (coluna)\n",
        "    max = np.max(X, axis=0)\n",
        "\n",
        "    # Evita divisão por zero\n",
        "    range_ = max - min\n",
        "    range_[range_ == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - min) / range_\n",
        "\n",
        "    return X_norm, min, max\n"
      ],
      "metadata": {
        "id": "mQHK6kb5kfZP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gradient_descent_multi.py"
      ],
      "metadata": {
        "id": "x84o3R7akf6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/gradient_descent_multi.py\n",
        "\"\"\"\n",
        "@file gradient_descent_multi.py\n",
        "@brief Performs gradient descent for multivariate regression.\n",
        "@details Este módulo contém uma função para executar o gradiente descendente\n",
        "          para regressão linear multivariada, atualizando os parâmetros θ\n",
        "          iterativamente para minimizar a função de custo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent_multi(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        error = X @ theta - y\n",
        "        gradient = (1 / m) * (X.T @ error)\n",
        "        theta = theta - alpha * gradient\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "\n",
        "def gradient_descent_multi_with_history(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ,\n",
        "    armazenando também o histórico de θ.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    n = X.shape[1]\n",
        "    J_history = np.zeros(num_iters)\n",
        "    theta_history = np.zeros((num_iters + 1, n))\n",
        "    theta_history[0] = theta.copy()\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        error = X @ theta - y\n",
        "        gradient = (1 / m) * (X.T @ error)\n",
        "        theta = theta - alpha * gradient\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "        theta_history[i + 1] = theta.copy()\n",
        "\n",
        "    return theta, J_history, theta_history\n"
      ],
      "metadata": {
        "id": "hBYU1SMnkgeJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normal_eqn.py"
      ],
      "metadata": {
        "id": "s4DHl37xkgyI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fCWyRs-QkdWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##regressao-multivariada-ex.py"
      ],
      "metadata": {
        "id": "GX2dq4Ptlk3v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v584t9AdlkmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}