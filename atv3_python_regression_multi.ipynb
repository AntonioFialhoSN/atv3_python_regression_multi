{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPpP8VXul4QoMivnRWB3P7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonioFialhoSN/atv3_python_regression_multi/blob/main/atv3_python_regression_multi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Redes Neurais"
      ],
      "metadata": {
        "id": "VHwGt09blz43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute_cost_multi.py\n"
      ],
      "metadata": {
        "id": "pzoPXcVzkdoY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m6TJVXzWkRgr"
      },
      "outputs": [],
      "source": [
        "# Functions/compute_cost_multi.py\n",
        "\"\"\"\n",
        "@file compute_cost_multi.py\n",
        "@brief Computes the cost for multivariate linear regression.\n",
        "@details Este módulo contém uma função para calcular o custo de um modelo de regressão linear\n",
        "          multivariada utilizando a função de custo de erro quadrático médio.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_cost_multi(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calcula o custo para regressão linear multivariada.\n",
        "\n",
        "    A função de custo é definida como:\n",
        "        J(θ) = (1 / (2m)) * (Xθ - y)ᵀ (Xθ - y)\n",
        "\n",
        "    :param (ndarray) X: Matriz de features incluindo o termo de intercepto (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :param (ndarray) theta: Vetor de parâmetros (shape: n+1,).\n",
        "    :return (float): Valor do custo calculado.\n",
        "    \"\"\"\n",
        "    # get the number of training examples\n",
        "    m = y.shape[0]\n",
        "\n",
        "    # compute the predictions using the linear model by formula h(θ) = X @ θ\n",
        "    predictions = X @ theta\n",
        "\n",
        "    # compute the error vector between predictions and actual values\n",
        "    errors = predictions - y\n",
        "\n",
        "    # compute the cost as the mean squared error cost function\n",
        "    cost = (1 / (2 * m)) * np.dot(errors, errors)\n",
        "\n",
        "    return cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##features_normalize.py"
      ],
      "metadata": {
        "id": "K8n23IYtkelf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/feature_normalize.py\n",
        "\"\"\"\n",
        "@file features_normalizes.py\n",
        "@brief Funções para normalização de features em datasets.\n",
        "@details Este módulo contém funções para normalizar as features de um dataset\n",
        "          utilizando diferentes abordagens, como média e desvio padrão, ou\n",
        "          mínimo e máximo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def features_normalize_by_std(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para média zero e desvio padrão unitário.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - mu) / sigma\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - mu é o vetor de médias (1 x n) de cada feature.\n",
        "        - sigma é o vetor de desvios padrão (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tripla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - mu (ndarray): Vetor com as médias de cada feature.\n",
        "        - sigma (ndarray): Vetor com os desvios padrão de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula a média de cada feature (coluna)\n",
        "    mu = np.mean(X, axis=0)\n",
        "\n",
        "    # Calcula o desvio padrão de cada feature (coluna)\n",
        "    sigma = np.std(X, axis=0)\n",
        "\n",
        "    # Evita divisão por zero\n",
        "    sigma[sigma == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return X_norm, mu, sigma\n",
        "\n",
        "\n",
        "def features_normalizes_by_min_max(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para o intervalo [0, 1] utilizando o mínimo e o máximo.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - min) / (max - min)\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - min é o vetor de mínimos (1 x n) de cada feature.\n",
        "        - max é o vetor de máximos (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tupla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - min (ndarray): Vetor com os valores mínimos de cada feature.\n",
        "        - max (ndarray): Vetor com os valores máximos de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula o mínimo de cada feature (coluna)\n",
        "    min = np.min(X, axis=0)\n",
        "\n",
        "    # Calcula o máximo de cada feature (coluna)\n",
        "    max = np.max(X, axis=0)\n",
        "\n",
        "    # Evita divisão por zero\n",
        "    range_ = max - min\n",
        "    range_[range_ == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - min) / range_\n",
        "\n",
        "    return X_norm, min, max\n"
      ],
      "metadata": {
        "id": "mQHK6kb5kfZP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gradient_descent_multi.py"
      ],
      "metadata": {
        "id": "x84o3R7akf6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/gradient_descent_multi.py\n",
        "\"\"\"\n",
        "@file gradient_descent_multi.py\n",
        "@brief Performs gradient descent for multivariate regression.\n",
        "@details Este módulo contém uma função para executar o gradiente descendente\n",
        "          para regressão linear multivariada, atualizando os parâmetros θ\n",
        "          iterativamente para minimizar a função de custo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent_multi(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        error = X @ theta - y\n",
        "        gradient = (1 / m) * (X.T @ error)\n",
        "        theta = theta - alpha * gradient\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "\n",
        "def gradient_descent_multi_with_history(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ,\n",
        "    armazenando também o histórico de θ.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    n = X.shape[1]\n",
        "    J_history = np.zeros(num_iters)\n",
        "    theta_history = np.zeros((num_iters + 1, n))\n",
        "    theta_history[0] = theta.copy()\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        error = X @ theta - y\n",
        "        gradient = (1 / m) * (X.T @ error)\n",
        "        theta = theta - alpha * gradient\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "        theta_history[i + 1] = theta.copy()\n",
        "\n",
        "    return theta, J_history, theta_history\n"
      ],
      "metadata": {
        "id": "hBYU1SMnkgeJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normal_eqn.py"
      ],
      "metadata": {
        "id": "s4DHl37xkgyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/normal_eqn.py\n",
        "\"\"\"\n",
        "@file normal_eqn.py\n",
        "@brief Calcula os parâmetros θ usando a Equação Normal.\n",
        "@details Este módulo contém uma função para calcular os parâmetros de um modelo\n",
        "          de regressão linear utilizando a equação normal.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normal_eqn(X, y):\n",
        "    \"\"\"\n",
        "    Resolve os parâmetros θ utilizando a equação normal.\n",
        "\n",
        "    A equação normal é definida como:\n",
        "        θ = (XᵀX)⁻¹ Xᵀ y\n",
        "\n",
        "    :param (ndarray) X: Matriz de features com bias, onde cada linha é uma amostra\n",
        "                        e cada coluna é uma feature (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :return (ndarray): Vetor de parâmetros θ (shape: n+1,).\n",
        "    \"\"\"\n",
        "    # Calcula os parâmetros θ utilizando a equação normal\n",
        "    # A equação normal é uma solução fechada para o problema de regressão linear\n",
        "    # que minimiza a soma dos erros quadráticos entre as previsões e os valores reais\n",
        "    # Implemente aqui a equação normal descrita na docstring. Use a função np.linalg.pinv\n",
        "    # para calcular a pseudo-inversa de uma matriz, que é útil quando a matriz não é quadrada\n",
        "    # ou não é invertível.\n",
        "    # A pseudo-inversa é uma generalização da inversa de uma matriz e pode ser usada para resolver\n",
        "    # sistemas de equações lineares que não têm uma solução única ou que são mal condicionados.\n",
        "    return np.linalg.pinv(X.T @ X) @ X.T @ y\n"
      ],
      "metadata": {
        "id": "fCWyRs-QkdWf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##regressao-multivariada-ex.py"
      ],
      "metadata": {
        "id": "GX2dq4Ptlk3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# regressao-multivariada-ex.py\n",
        "\"\"\"\n",
        "@file regressao-multivariada-ex.py\n",
        "@brief Multivariate linear regression exercise with gradient descent and normal equation.\n",
        "@details Este script executa um fluxo de trabalho completo para regressão linear multivariada,\n",
        "          incluindo normalização de features, cálculo de parâmetros via gradiente descendente\n",
        "          e equação normal, além de comparação de custos.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def costs_from_history(X_b: np.ndarray, y: np.ndarray, thetas: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Calcula o custo J(θ) para cada θ em *thetas*.\"\"\"\n",
        "    return np.array([compute_cost_multi(X_b, y, th) for th in thetas])\n",
        "\n",
        "def main():\n",
        "    # 1) Cria pasta de figuras\n",
        "    os.makedirs(\"Figures\", exist_ok=True)\n",
        "\n",
        "    # 2) Carrega dados\n",
        "    data = np.loadtxt('Data/ex1data2.txt', delimiter=',')\n",
        "    X = data[:, 0:2]\n",
        "    y = data[:, 2]\n",
        "    m = len(y)\n",
        "\n",
        "    print('Primeiros 10 exemplos de treinamento:')\n",
        "    print(np.column_stack((X[:10], y[:10])))\n",
        "\n",
        "    # 3) Normaliza features\n",
        "    X_norm, mu, sigma = features_normalize_by_std(X)\n",
        "    X_b = np.column_stack((np.ones(m), X_norm))  # X para GD\n",
        "\n",
        "    print('\\nParâmetros de normalização:')\n",
        "    print(f'Média (mu): {mu}')\n",
        "    print(f'Desvio Padrão (sigma): {sigma}')\n",
        "\n",
        "    # 4) Gradient Descent Multivariado\n",
        "    alpha = 0.01\n",
        "    num_iters = 400\n",
        "    theta_gd = np.zeros(3)\n",
        "    theta_gd, J_history = gradient_descent_multi(X_b, y, theta_gd, alpha, num_iters)\n",
        "    print('\\nTheta via Gradient Descent:')\n",
        "    print(theta_gd)\n",
        "\n",
        "    # 4a) Plot de convergência (GD)\n",
        "    plt.figure()\n",
        "    plt.plot(np.arange(1, num_iters + 1), J_history, 'b-', linewidth=2)\n",
        "    plt.xlabel('Iteração')\n",
        "    plt.ylabel('Custo J(θ)')\n",
        "    plt.title('Convergência do Gradiente (Multivariada)')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('Figures/convergencia_custo_multi.png', dpi=300, bbox_inches='tight')\n",
        "    plt.savefig('Figures/convergencia_custo_multi.svg', format='svg', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 5) Predição com GD\n",
        "    example = np.array([1650, 3])\n",
        "    example_norm = (example - mu) / sigma\n",
        "    x_pred = np.concatenate(([1], example_norm))\n",
        "    price_gd = x_pred @ theta_gd\n",
        "    print(f'\\nPreço previsto (GD) para [1650,3]: ${price_gd:.2f}')\n",
        "\n",
        "    # 6) Equação Normal\n",
        "    X_ne = np.column_stack((np.ones(m), X))\n",
        "    theta_ne = normal_eqn(X_ne, y)\n",
        "    example = np.array([1, 1650, 3])\n",
        "    price_ne = example @ theta_ne\n",
        "    print('\\nTheta via Equação Normal:')\n",
        "    print(theta_ne)\n",
        "    print(f'Preço previsto (NE) para [1650,3]: ${price_ne:.2f}')\n"
      ],
      "metadata": {
        "id": "v584t9AdlkmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}